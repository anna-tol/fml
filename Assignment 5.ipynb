{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fundamentals of Machine Learning <br>\n",
    "Master Data-Driven Design <br>\n",
    "University of Applied Science Utrecht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words model and Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-words is an algorithm that counts the occurance of words in a text. Since it only represents the frequency of each individual word, context is lost. Information regarding the order or structure is discarded. For example: words that belong together, such as 'New York' ('New' and 'York'), are seen as seperate words and countes seperately.\n",
    "\n",
    "The Naive Bayes classifier, on the other hand, is an algorithm that classifies features. The algorithm sees these feautures as seperate and independent. It is called naive because this is most often not the case. Using the Naive Bayes algorithm, we can predict whether a review is positive or neutral/negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing stuff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the dataset\n",
    "data = pd.read_csv('Assignment text mining - data clothing reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discarding all reviews other than dresses\n",
    "data = data.loc[(data['Class Name'] == 'Dresses')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keeping review text and rating + nice and clean renaming\n",
    "data = data[['Review Text', 'Rating']]\n",
    "data = data.rename(columns = {'Review Text': 'review', 'Rating': 'rating'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data into neutral/negative and positive\n",
    "data['rating'] = data['rating'].map(\n",
    "    {0: 'neutral/negative', \n",
    "     1: 'neutral/negative',\n",
    "     2: 'neutral/negative',\n",
    "     3: 'neutral/negative',\n",
    "     4: 'positive',\n",
    "     5: 'positive'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the nothing\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>neutral/negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>neutral/negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I love this dress. i usually get an xs but it ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I'm 5\"5' and 125 lbs. i ordered the s petite t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review            rating\n",
       "1  Love this dress!  it's sooo pretty.  i happene...          positive\n",
       "2  I had such high hopes for this dress and reall...  neutral/negative\n",
       "5  I love tracy reese dresses, but this one is no...  neutral/negative\n",
       "8  I love this dress. i usually get an xs but it ...          positive\n",
       "9  I'm 5\"5' and 125 lbs. i ordered the s petite t...          positive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text to unicode\n",
    "text = data['review'].values.astype('U') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object with English stopwords and fit our text with the model\n",
    "vect = CountVectorizer(stop_words = 'english')\n",
    "vect = vect.fit(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the features\n",
    "doc_feat = vect.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (2, 8)\t1\n",
      "  (20, 38)\t1\n",
      "  (21, 4)\t1\n",
      "  (21, 45)\t1\n",
      "  (22, 12)\t1\n",
      "  (25, 40)\t1\n",
      "  (34, 12)\t2\n",
      "  (38, 31)\t1\n"
     ]
    }
   ],
   "source": [
    "# print to show we got something\n",
    "print(doc_feat[0 : 50, 0 : 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document-feature matrix as x and rating as y\n",
    "x = doc_feat\n",
    "y = data['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a test and training set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "nb = nb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_test_p = nb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8508676789587852"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the accuracy of those predictions\n",
    "nb.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model is 85%, which is quite okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neutral/negative pred</th>\n",
       "      <th>positive pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>neutral/negative</th>\n",
       "      <td>301</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>107</td>\n",
       "      <td>1268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  neutral/negative pred  positive pred\n",
       "neutral/negative                    301            168\n",
       "positive                            107           1268"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a confusion matrix\n",
    "conf_matr = confusion_matrix(y_test, y_test_p)\n",
    "pd.DataFrame(conf_matr, index = ['neutral/negative', 'positive'], \n",
    "             columns = ['neutral/negative pred', 'positive pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "neutral/negative       0.74      0.64      0.69       469\n",
      "        positive       0.88      0.92      0.90      1375\n",
      "\n",
      "        accuracy                           0.85      1844\n",
      "       macro avg       0.81      0.78      0.79      1844\n",
      "    weighted avg       0.85      0.85      0.85      1844\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print classification report\n",
    "print(classification_report(y_test, y_test_p, nb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_test_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>neutral/negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>neutral/negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>neutral/negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>neutral/negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>neutral/negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>neutral/negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>positive</td>\n",
       "      <td>neutral/negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>positive</td>\n",
       "      <td>neutral/negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>neutral/negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>neutral/negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                y_test          y_test_p\n",
       "23    neutral/negative          positive\n",
       "52    neutral/negative          positive\n",
       "383   neutral/negative          positive\n",
       "480   neutral/negative          positive\n",
       "506   neutral/negative          positive\n",
       "533   neutral/negative          positive\n",
       "560           positive  neutral/negative\n",
       "734           positive  neutral/negative\n",
       "755   neutral/negative          positive\n",
       "1140  neutral/negative          positive"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find wrong predictions to check\n",
    "test_data = pd.DataFrame({'y_test' : y_test, 'y_test_p' : y_test_p})\n",
    "test_data[test_data['y_test'] != test_data['y_test_p']].sort_index().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfect dress for hot, humid, sticky weather.\n",
      "negative: 0.012275032134469864, positive: 0.9877249678655337\n"
     ]
    }
   ],
   "source": [
    "# a neutral/negative review mistaken for a positive one\n",
    "prob = nb.predict_proba(x[23])\n",
    "print(data.iloc[23]['review'])\n",
    "print(f'negative: {prob[0, 0]}, positive: {prob[0, 1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dress has potential, but it didn't work for me. it runs true to size to a little big, i ordered medium, my usual size for maeve). as for length it fit me as the model (5'9\"). the reason i'm not keeping it is that i wish it had some darts in the back to help define the waist a bit,\n",
      "negative: 0.05943374614029698, positive: 0.9405662538597088\n"
     ]
    }
   ],
   "source": [
    "# another neutral/negative review mistaken for a positive one\n",
    "prob = nb.predict_proba(x[52])\n",
    "print(data.iloc[52]['review'])\n",
    "print(f'negative: {prob[0, 0]}, positive: {prob[0, 1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love this dress. i'd get it in both colors if i could! the cut and fit is beautiful, i'd suggest sizing down if you feel like it's too boxy or lacks shape. the bottom skirt is round enough to where you can twirl ( #1 thing to look for in a dress ;) ) and it's just overall a classic pretty dress. my only complaint is that the overlay cut out seems a little bit delicate and i'm afraid it will be ruined after a few wears but it seems to be holding up fine so far and isn't incredibly delicate like\n",
      "negative: 0.06752837329433599, positive: 0.932471626705665\n"
     ]
    }
   ],
   "source": [
    "# a positive review mistaken for a neutral/negative one\n",
    "prob = nb.predict_proba(x[560])\n",
    "print(data.iloc[560]['review'])\n",
    "print(f'negative: {prob[0, 0]}, positive: {prob[0, 1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cases above the model is wrong due to the way humans use language. In the first case sarcasm was used, which the model obviously cannot detect. In the second case it is the lack of context that the model is missing: words like potential, true, fit, define, waist, etc. make it seem like the reviewer was positive about the dress. The thirds case could also benefit from context: the reviewer names suggestions for others and small details that (s)he did not like that make it seem like a negative review to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find most informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral/negative: really, small, love, look, fit, just, fabric, size, like, dress\n",
      "positive: flattering, fabric, just, great, like, wear, fit, love, size, dress\n"
     ]
    }
   ],
   "source": [
    "# get all features\n",
    "feature_names = vect.get_feature_names()\n",
    "\n",
    "# get top 10\n",
    "top = 10\n",
    "\n",
    "for i, class_label in enumerate(nb.classes_):\n",
    "    \n",
    "    # find top features (numbers)\n",
    "    results = np.argsort(nb.feature_log_prob_[i])[-top:]\n",
    "    \n",
    "    # translate numbers to corresponding words\n",
    "    print(\"%s: %s\" % (class_label, ', '.join(feature_names[j] for j in results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't find these very informative since they overlap quite a lot, but ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you cancel out the overlap:\n",
    "* really, small, look\n",
    "* flattering, great, wear"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
